body,context_ChatGPT model,context_temperature,context_ChatGPT token usage,context_prompt,timestamp
"```json
LIST OF TAGS: [""DynamoDB"", ""Performance"", ""Scalability""]
```",gpt-4-1106-preview,0.7,"CompletionUsage(completion_tokens=22, prompt_tokens=1594, total_tokens=1616)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Load Testing Makes Your DynamoDB Table Better

In DynamoDB, load testing actually makes your table more ready and performant for production.
Yes, literally.
It's not just that you get peace of mind during the actual production run because you did proper load testing. No, your DynamoDB table literally gets better. Let's go through exactly why this is, and along the way learn some cool things about DynamoDB and how it operates under the hood.
O(1) Routers
If this isn't the first time you're hearing about DynamoDB, you've no doubt heard the magical sales pitch about how DynamoDB can scale up to planet-scale while still retaining predictable performance throughout. Think Amazon.com levels of traffic during Black Friday - because DynamoDB is literally what powers their backend.
This isn't because DynamoDB runs on some sort of alien-tech mega-server. No, instead DynamoDB runs on a fleet of servers.
Data in DynamoDB, and requests to DynamoDB, are actually stored and served by a huge collection of servers, and the way these data and requests are split is the interesting bit. The way it's done is what gives DynamoDB its consistent, predictable performance scaling.
Every request to DynamoDB (read and write) is handled by a request router. The router understands metadata about your table, and knows which items belong to which server. So when you ask it to retrieve an item, or ask it to write an item, it knows which server in the DynamoDB fleet of servers it should direct the request to.
It doesn't matter how many servers actually hold your data. There can be only 2, if you have a small data set. There can be a hundred, if you had a gigantic amount of data. It doesn't matter. The router figures out just as quickly which server to direct a request - in CS terms, it does the work in O(1) time (i.e., consistent time, regardless of input size).
This is how DynamoDB essentially takes the same amount of time to retrieve that piece of data you asked for, whether your database was 10GB or 10TB. The request router just immediately zeroes in on the right server in O(1) time, and that server only holds a fraction of your data set - even if your total data size is 10TB, it never really needs to inspect the vast majority of your data, so it isn't slowed down at all.
Partitions
Ok, that sounds absolutely magical. How does DynamoDB actually make this O(1) router algorithm work?
First, let's introduce the concept of partitions. DynamoDB partitions your data across its fleet of servers, and this is why each server in the DynamoDB fleet holds only a part of your total data set.
Your data is stored across different partitions within the DynamoDB fleet. The job of the request router is to quickly figure out which partition (and therefore, which server) is responsible for the data (whether to write to, or read from).
And the way the router does this is simple - it takes your item's partition key (this is the sole required element in a DynamoDB primary key), and hashes it with an undisclosed hash function. Hashing is necessary to make sure the data is more uniformly-distributed across the different partitions, regardless of the actual original values in the partition keys.
The resulting hash value determines which partition the data belongs to, and the router immediately knows which server to direct it to. Since it is essentially a one-step operation (take the partition key and hash it), the router is just as quick to determine which partition it needs to talk to, regardless of whether your data has only 2 partitions or 100 partitions.
Partition Splitting
To maintain its consistent performance, DynamoDB has to ensure that partitions are only ever so large - it can't, for example, have a partition that has 1TB of data, otherwise it'll run into the same problems as your typical RDBMS, where the bigger it gets, the slower it performs. The whole point of partitioning is to make sure each node in the fleet only ever deals with a manageable partition size, and there will just be a ton of them as needed to handle ever-increasing data sizes.
DynamoDB's individual servers also only have so much compute performance, so an individual partition can only be so fast. If a user needs more performance out of DynamoDB, then DynamoDB would also need to have more partitions (ergo, more individual servers).
To make this work, DynamoDB does automatic partition splitting. If a partition grows too big, or if provisioned capacity exceeds the capacity of existing partitions, DynamoDB transparently splits the partitions as needed.
In a nutshell, since more partitions = more servers, and more servers = better performance, therefore more partitions = better performance.
Now, we're getting closer to this article's premise - that load testing your DynamoDB table will make it literally better for production.
Load Testing and Benchmarking
Say you're readying your application (powered by DynamoDB) for a huge production launch. Like a good IT professional, you decide you really want to make sure you're ready for prod, so you decide to battle test your infrastructure.
You and your team decide to do load testing and benchmarking, close to what you expect for the prod launch.
From DynamoDB's perspective, this load testing is indistinguishable from production activity. It can't tell the difference. All it sees is that more and more data are getting added, and more and more requests are getting directed to different partitions.
During that testing, your DynamoDB table ended up with a ton of new data, filling existing partitions, and forcing DynamoDB to transparently split partitions automatically, ending up with a host of new partitions for your data.
Now, when DynamoDB splits a partition, it is permanent. It doesn't matter, for example, that you will delete all load testing data from your DynamoDB table before the prod launch (whether manually or through TTL). The split partitions will remain.
So now you have more partitions than when you started with. And more partitions = better performance.
When the prod launch actually happens, that DynamoDB table you exercised is literally more ready and will literally perform better and more capable of scaling against your expected prod load. Remember this the next time you need to prep for a high-volume launch involving your DynamoDB-powered application!
```
",2023-11-13 18:27:20
"```json
[""DynamoDB"", ""Performance"", ""Scalability""]
```",gpt-4-1106-preview,0.7,"CompletionUsage(completion_tokens=17, prompt_tokens=1594, total_tokens=1611)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Load Testing Makes Your DynamoDB Table Better

In DynamoDB, load testing actually makes your table more ready and performant for production.
Yes, literally.
It's not just that you get peace of mind during the actual production run because you did proper load testing. No, your DynamoDB table literally gets better. Let's go through exactly why this is, and along the way learn some cool things about DynamoDB and how it operates under the hood.
O(1) Routers
If this isn't the first time you're hearing about DynamoDB, you've no doubt heard the magical sales pitch about how DynamoDB can scale up to planet-scale while still retaining predictable performance throughout. Think Amazon.com levels of traffic during Black Friday - because DynamoDB is literally what powers their backend.
This isn't because DynamoDB runs on some sort of alien-tech mega-server. No, instead DynamoDB runs on a fleet of servers.
Data in DynamoDB, and requests to DynamoDB, are actually stored and served by a huge collection of servers, and the way these data and requests are split is the interesting bit. The way it's done is what gives DynamoDB its consistent, predictable performance scaling.
Every request to DynamoDB (read and write) is handled by a request router. The router understands metadata about your table, and knows which items belong to which server. So when you ask it to retrieve an item, or ask it to write an item, it knows which server in the DynamoDB fleet of servers it should direct the request to.
It doesn't matter how many servers actually hold your data. There can be only 2, if you have a small data set. There can be a hundred, if you had a gigantic amount of data. It doesn't matter. The router figures out just as quickly which server to direct a request - in CS terms, it does the work in O(1) time (i.e., consistent time, regardless of input size).
This is how DynamoDB essentially takes the same amount of time to retrieve that piece of data you asked for, whether your database was 10GB or 10TB. The request router just immediately zeroes in on the right server in O(1) time, and that server only holds a fraction of your data set - even if your total data size is 10TB, it never really needs to inspect the vast majority of your data, so it isn't slowed down at all.
Partitions
Ok, that sounds absolutely magical. How does DynamoDB actually make this O(1) router algorithm work?
First, let's introduce the concept of partitions. DynamoDB partitions your data across its fleet of servers, and this is why each server in the DynamoDB fleet holds only a part of your total data set.
Your data is stored across different partitions within the DynamoDB fleet. The job of the request router is to quickly figure out which partition (and therefore, which server) is responsible for the data (whether to write to, or read from).
And the way the router does this is simple - it takes your item's partition key (this is the sole required element in a DynamoDB primary key), and hashes it with an undisclosed hash function. Hashing is necessary to make sure the data is more uniformly-distributed across the different partitions, regardless of the actual original values in the partition keys.
The resulting hash value determines which partition the data belongs to, and the router immediately knows which server to direct it to. Since it is essentially a one-step operation (take the partition key and hash it), the router is just as quick to determine which partition it needs to talk to, regardless of whether your data has only 2 partitions or 100 partitions.
Partition Splitting
To maintain its consistent performance, DynamoDB has to ensure that partitions are only ever so large - it can't, for example, have a partition that has 1TB of data, otherwise it'll run into the same problems as your typical RDBMS, where the bigger it gets, the slower it performs. The whole point of partitioning is to make sure each node in the fleet only ever deals with a manageable partition size, and there will just be a ton of them as needed to handle ever-increasing data sizes.
DynamoDB's individual servers also only have so much compute performance, so an individual partition can only be so fast. If a user needs more performance out of DynamoDB, then DynamoDB would also need to have more partitions (ergo, more individual servers).
To make this work, DynamoDB does automatic partition splitting. If a partition grows too big, or if provisioned capacity exceeds the capacity of existing partitions, DynamoDB transparently splits the partitions as needed.
In a nutshell, since more partitions = more servers, and more servers = better performance, therefore more partitions = better performance.
Now, we're getting closer to this article's premise - that load testing your DynamoDB table will make it literally better for production.
Load Testing and Benchmarking
Say you're readying your application (powered by DynamoDB) for a huge production launch. Like a good IT professional, you decide you really want to make sure you're ready for prod, so you decide to battle test your infrastructure.
You and your team decide to do load testing and benchmarking, close to what you expect for the prod launch.
From DynamoDB's perspective, this load testing is indistinguishable from production activity. It can't tell the difference. All it sees is that more and more data are getting added, and more and more requests are getting directed to different partitions.
During that testing, your DynamoDB table ended up with a ton of new data, filling existing partitions, and forcing DynamoDB to transparently split partitions automatically, ending up with a host of new partitions for your data.
Now, when DynamoDB splits a partition, it is permanent. It doesn't matter, for example, that you will delete all load testing data from your DynamoDB table before the prod launch (whether manually or through TTL). The split partitions will remain.
So now you have more partitions than when you started with. And more partitions = better performance.
When the prod launch actually happens, that DynamoDB table you exercised is literally more ready and will literally perform better and more capable of scaling against your expected prod load. Remember this the next time you need to prep for a high-volume launch involving your DynamoDB-powered application!
```
",2023-11-13 18:28:03
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1925, total_tokens=1936)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:31:53
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1925, total_tokens=1936)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:32:01
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1925, total_tokens=1936)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:32:11
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1925, total_tokens=1936)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:32:16
"```json
[""Security"", ""Serverless"", ""DevOps""]
```",gpt-3.5-turbo-1106,0.7,"CompletionUsage(completion_tokens=15, prompt_tokens=1925, total_tokens=1940)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:32:38
"```json
[""Serverless"", ""Security"", ""DevOps""]
```",gpt-3.5-turbo-1106,0.7,"CompletionUsage(completion_tokens=15, prompt_tokens=1925, total_tokens=1940)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:32:46
"```json
[""Security"", ""Serverless"", ""DevOps""]
```",gpt-3.5-turbo-1106,0.7,"CompletionUsage(completion_tokens=15, prompt_tokens=1925, total_tokens=1940)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:32:49
"
```json
[""Security"", ""Serverless"", ""DevOps""]
```",gpt-3.5-turbo-1106,0.7,"CompletionUsage(completion_tokens=16, prompt_tokens=1925, total_tokens=1941)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:33:30
"```json
[""Serverless"", ""Security"", ""Lambda""]
```",gpt-4-1106-preview,0.7,"CompletionUsage(completion_tokens=14, prompt_tokens=1925, total_tokens=1939)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:33:52
"```json
[""Serverless"", ""Security"", ""DevOps""]
```",gpt-4-1106-preview,0.7,"CompletionUsage(completion_tokens=15, prompt_tokens=1925, total_tokens=1940)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:33:56
"```json
[""Serverless"", ""Security"", ""DevOps""]
```",gpt-4-1106-preview,0.7,"CompletionUsage(completion_tokens=15, prompt_tokens=1925, total_tokens=1940)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```

Title: Why Serverless Is Inherently More Secure Than Containers

Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs.
Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless?
Nope, not really.
And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security.
Heavy Lifting in Serverless vs Containers
Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself.
For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository.
Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem.
To VPC or not to VPC - that is the [factor]
A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud).
In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC.
No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs).
Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option.
Security is hard, and always harder when you have to do more
These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers.
Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task.
In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden.
If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group.
What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration.
Wrap up
Serverless is inherently more secure than containers. This is due primarily to two big factors:
Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch.
Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.

Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more.
And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration.
If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful!
Security Report Links:
IBM 2020 Cloud Threat Landscape Report
IBM 2021 Cloud Threat Landscape Report
IBM 2022 Cloud Thread Landscape Report
Orca Security: State of Public Cloud Report 2022
Trend Micro: Untangling the Web of Cloud Security Threats
Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps

```
",2023-11-13 18:34:04
"```json
[""Serverless"", ""Security"", ""Governance""]
```",gpt-4-1106-preview,0.7,"CompletionUsage(completion_tokens=15, prompt_tokens=1913, total_tokens=1928)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud. Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 10:46:24
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1913, total_tokens=1924)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud. Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 10:47:55
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1913, total_tokens=1924)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud. Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 10:48:00
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1913, total_tokens=1924)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud. Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 10:48:51
"```json
[""Security"", ""Serverless"", ""Containers""]
```",gpt-3.5-turbo-1106,0.7,"CompletionUsage(completion_tokens=14, prompt_tokens=1913, total_tokens=1927)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud. Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 10:51:38
"```json
[""Security"", ""Serverless"", ""DevOps""]
```",gpt-3.5-turbo-1106,0.7,"CompletionUsage(completion_tokens=15, prompt_tokens=1913, total_tokens=1928)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud. Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 10:51:42
"```json
[""Serverless"", ""Security"", ""Containers""]
```",gpt-3.5-turbo-1106,0.7,"CompletionUsage(completion_tokens=14, prompt_tokens=1915, total_tokens=1929)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers  Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.  Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 11:18:17
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1915, total_tokens=1926)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers  Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.  Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 11:19:08
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1915, total_tokens=1926)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers  Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.  Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 11:19:12
"[""Serverless"", ""Security"", ""Governance""]",ft:gpt-3.5-turbo-1106:personal::8KMcfHLQ,0.7,"CompletionUsage(completion_tokens=11, prompt_tokens=1915, total_tokens=1926)","
Analyze the article and identify three tags that best summarize its main topics
ARTICLE TEXT:
```
Title: Why Serverless Is Inherently More Secure Than Containers  Almost a year ago, I had an interesting convo with a highly-technical and accomplished person. She just happened to not be a serverless expert. When we started talking about serverless, she agreed it's the future and then went on to paint a picture of a world where everything (every workload) is just running on containers - no more VMs. Hmmm… ok, not quite what I had in mind. She mixed up serverless and containers, but for good reason - containers can be run serverlessly, such as through AWS Fargate or by using custom container images for Lambda. When you run containers serverlessly this way, isn't that pretty much the same thing as native serverless? Nope, not really. And I'm not just trying to be nit-picky. Nuance matters, and in this case, it is especially true because it involves security. Heavy Lifting in Serverless vs Containers Although a serverless setup and a container setup will both provide a lot of beneficial abstractions compared to traditional VM-based workload setups, a container-based workload abstracts far less (i.e, through an orchestration tool, be it something like Kubernetes, Docker Swarm, or cloud-native ones like ECS and EKS in AWS). And the less is abstracted, the more work you still have to do yourself. For example, in a typical container workload, you are pretty much still responsible for a lot of maintenance and patching - not just of your actual source code, but of the OS itself (i.e., the container image which has a base OS and necessary utilities and components, aside from just your code). Oh, yeah, you have to manage container images now, so you maintain not just a repository of code (like your GitHub repos), but also an image repository. Yikes. In the real serverless world, you maintain the Git repo of your code, and that's it. Image patching? Maintaining an image repository? Not your problem. To VPC or not to VPC - that is the [factor] A real biggie of a difference is whether you even need to bother with a VPC (AWS VPC = Virtual Private Cloud). In a native serverless workload - say, you have applications running on API Gateway, S3, Lambda, DynamoDB, with Glue+Athena for some native serverless analytics and heavy reporting - you don't even have to bother with a VPC. You have a modern, feature-rich, web-based system doing OLTP and OLAP, and you never have to configure or maintain a single VPC. No VPC means no related NACLs and security groups to configure and maintain (and then re-configure / debug / test each time new workloads are deployed with slightly different networking needs). Typical container workloads? Sorry, they literally run in VPCs. You still have to do all that heavy lifting yourself - including things beyond just NACLs and security groups, like thinking about private vs public subnets, NAT or egress-only gateways, etc. That's more opportunity for someone in the team to make a mistake and misconfigure a critical networking option. Security is hard, and always harder when you have to do more These two significant differences between real serverless vs container-based workloads (whether these container-based workloads are run serverlessly or not) lead to my general rule: Serverless is inherently more secure than containers. Patching - especially security patching - is a thankless, and thus often-overlooked, task. In Orca Security's 2022 State of Public Cloud Report (see report links at end of the article), the securty firm found that 78% of attacks start with the exploitation of a known vulnerability. Totally makes sense, not surprising. What's probably more surprising is they found that the average VM and container image had at least 50 known vulnerabilities (CVEs) in one year. Or, maybe it isn't really that surprising, since patching - especially security patching - is a thankless, and thus often-overlooked, task. In serverless land, this is not your problem. OS and platform stack patching (anything that isn't in your codebase) is offloaded to the hyperscaler. You're golden. If you're doing container-based workloads, well, you're either in deep trouble because you have assets that are similar to the ""average VM / container"" that Orca Security found (i.e., riddled with known vulnerabilities), or you already have a heavy process that continually scans, patches, and updates your image repository and deploys updates to containers. And you have to keep doing all that work, and all the costs associated with it (in terms of tooling and related licenses, as well as man-hours alloted to the relevant processes), just to avoid being in Orca Security's ""average"" group. What's more, almost all security firms looking at the Cloud are in general agreement about the most common cause of breaches (for example, annual reports from Trend Micro, IBM and Orca Security for the past few years; links at end of this article). And this most common cause is resource misconfiguration. This is why I was harping about VPC vs no VPC a while ago - that's a great example of heavy lifting that will have a ton of security-impacting configuration that you could easily mess up. When you offload all of that to the hyperscaler, you have essentially increased your security posture for free: You have less surface area for potential resource misconfiguration. Wrap up Serverless is inherently more secure than containers. This is due primarily to two big factors: Serverless offloads a lot of patching and maintenance work to the hyperscaler, so there's less for you to forget to patch. Serverless greatly reduces your surface area for potential resource misconfiguration, which is the most common cause of breaches in the cloud.  Of course, not everything is perfect in the serverless space. Last week, I wrote a semi-rant about serverless. But even though that includes an item about hyperscaler stewardship that could be better, I'm still 100% comfortable evangelizing serverless as the inherently more secure way to run modern workloads. This is because, if you study the annual reports of security vendors and research firms, no one is really citing ""hyperscaler vulnerability"" as one of the top causes of data/security breaches. See the links below and have a read yourself. I'm not trying to sweep any hyperscaler vulnerability as unimportant - I'm just being practical and trying to urge people into focusing on the things that matter more. And to be 100% clear, I'm not just making a ""Lambda functions vs containers"" comparison. I do mean ""serverless"" as in the full serverless stack, such as workloads running purely on serverless services like API Gateway, S3, Lambda, DynamoDB, and even Glue (Jobs + Data Catalog) and Athena. All these are awesome serverless services (DynamoDB and Athena are my personal favorites), with all of the advantages I've been talking about so far when it comes to catching up on security patching / CVEs, and in reducing your surface area for potential resource misconfiguration. If you are part of a team thinking about modernizing workloads and security is a top concern, I hope you found this useful! Security Report Links: IBM 2020 Cloud Threat Landscape Report IBM 2021 Cloud Threat Landscape Report IBM 2022 Cloud Thread Landscape Report Orca Security: State of Public Cloud Report 2022 Trend Micro: Untangling the Web of Cloud Security Threats Dark Reading: Attackers Can Compromise Most Cloud Data in Just 3 Steps
```
",2023-11-15 11:19:30
